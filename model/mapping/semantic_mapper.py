from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime

class SemanticMapper:
    """
    SemanticMapper performs label-free semantic mapping of survey questions
    to high-level wellness dimensions using embedding similarity.

    Each wellness dimension is represented by a natural-language definition
    (a semantic prototype), typically generated by a large language model (LLM).
    Survey questions and dimension definitions are embedded into the same
    vector space, and questions are assigned to one or more dimensions
    based on cosine similarity with a margin-based selection rule.

    This class is designed to:
        - Support multiple dimension sets generated by different LLMs
        - Preserve question metadata (qid, dataset) during mapping
        - Enable interpretable, theory-aligned baseline comparisons

    Typical usage:
        1. Initialize mapper with a sentence embedding model
        2. Set dimension definitions from one LLM
        3. Set survey questions from a DataFrame
        4. Map questions to dimensions with a similarity margin
    """

    def __init__(self, model: str = "all-MiniLM-L6-v2"):
        """
        Initialize the SemanticMapper.

        Args:
            model:
                Name of the SentenceTransformer model used to embed
                both questions and dimension definitions.
                This model is fixed across all dimension sets to ensure
                fair comparison between different LLM-generated definitions.
        """
        # Sentence embedding model (shared space for questions and dimensions)
        self.model = SentenceTransformer(model)


        # Dimension-related state
        self.dimensions: Optional[List[str]] = None          # Full dimension definitions
        self.dim_names: Optional[List[str]] = None           # Short dimension labels (e.g., "Emotional")
        self.dim_embeddings: Optional[np.ndarray] = None     # Embeddings of dimension definitions
        self.dimension_model_name: Optional[str] = None      # LLM that generated the dimensions

        # Question-related state (DataFrame-based)
        self.questions_df: Optional[pd.DataFrame] = None
        self.text_col: str = "text"
        self.qid_col: str = "qid"
        self.dataset_col: str = "dataset"
        self.question_embeddings: Optional[np.ndarray] = None

    def set_dimensions(
        self,
        dimensions: List[str],
        dimension_model_name: str,
    ) -> None:
        """
        Register and encode a set of wellness dimension definitions.

        Each call to this method corresponds to one dimension set,
        typically generated by a specific LLM using a fixed prompt.

        Args:
            dimensions:
                List of dimension definitions in the form
                "DimensionName: description".
            dimension_model_name:
                Identifier of the LLM that generated this dimension set
                (e.g., "ChatGPT-5.2", "Llama-4", "claude-sonnet-4.5").
        """
        self.dimensions = dimensions
        self.dimension_model_name = dimension_model_name

        # Extract short labels for interpretability and reporting
        self.dim_names = [d.split(":", 1)[0].strip() for d in dimensions]

        # Encode full dimension definitions as semantic prototypes
        self.dim_embeddings = self.model.encode(
            dimensions, convert_to_numpy=True
        )

    def set_questions_df(
        self,
        df: pd.DataFrame,
        text_col: str = "text",
        qid_col: str = "qid",
        dataset_col: str = "dataset",
    ) -> None:
        """
        Register survey questions and compute embeddings.

        Only the text column is used for embedding, while all other
        metadata columns (qid, dataset) are preserved for downstream
        analysis and reporting.

        Args:
            df:
                DataFrame containing survey questions.
            text_col:
                Column name containing question text.
            qid_col:
                Column name for question identifiers.
            dataset_col:
                Column name for dataset/source identifiers.
        """
        if text_col not in df.columns:
            raise ValueError(f"Missing column: {text_col}")

        self.text_col = text_col
        self.qid_col = qid_col
        self.dataset_col = dataset_col

        # Keep a copy to preserve metadata in outputs
        self.questions_df = df.copy()

        # Embed only the textual content of questions
        texts = self.questions_df[text_col].astype(str).tolist()
        self.question_embeddings = self.model.encode(
            texts, convert_to_numpy=True
        )

    def map_questions_to_dimensions(self, delta: float) -> pd.DataFrame:
        """
        Map each survey question to one or more wellness dimensions.

        For each question, cosine similarity is computed against all
        dimension prototypes. All dimensions whose similarity score
        lies within `delta` of the best-matching dimension are assigned,
        allowing multi-dimensional membership when concepts overlap.

        Args:
            delta:
                Margin threshold for multi-label assignment.
                Smaller values encourage single-label mappings,
                while larger values allow broader overlap.

        Returns:
            DataFrame containing:
                - question identifier (qid)
                - dataset/source
                - question text
                - dimension model name (LLM source)
                - assigned dimension names
                - corresponding similarity scores
        """
        if self.dim_embeddings is None or self.dim_names is None:
            raise ValueError(
                "Dimensions must be set before mapping (call set_dimensions)."
            )
        if self.questions_df is None or self.question_embeddings is None:
            raise ValueError(
                "Questions must be set before mapping (call set_questions_df)."
            )

        # Compute cosine similarity between questions and dimension prototypes
        sim_matrix = self._cosine_similarity_matrix(
            self.question_embeddings,
            self.dim_embeddings
        )

        dims_out, scores_out = [], []

        for i in range(len(self.questions_df)):
            scores = sim_matrix[i]
            idx = self._select_dimensions_by_margin(scores, delta)

            dims_out.append([self.dim_names[j] for j in idx])
            scores_out.append([float(scores[j]) for j in idx])

        # Construct output with preserved metadata
        out = self.questions_df.copy()
        out["dimension_model"] = self.dimension_model_name
        out["dimensions"] = dims_out
        out["scores"] = scores_out

        keep_cols = [
            c for c in [
                self.qid_col,
                self.dataset_col,
                self.text_col,
                "dimension_model",
            ]
            if c in out.columns
        ]
        keep_cols += ["dimensions", "scores"]

        return out[keep_cols]

    @staticmethod
    def _cosine_similarity_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:
        """
        Compute cosine similarity between two embedding matrices.

        Args:
            A:
                Matrix of shape (N, D), typically question embeddings.
            B:
                Matrix of shape (M, D), typically dimension embeddings.

        Returns:
            Similarity matrix of shape (N, M).
        """
        eps = 1e-12  # Numerical stability
        A_norm = A / (np.linalg.norm(A, axis=1, keepdims=True) + eps)
        B_norm = B / (np.linalg.norm(B, axis=1, keepdims=True) + eps)
        return np.dot(A_norm, B_norm.T)

    @staticmethod
    def _select_dimensions_by_margin(
        scores: np.ndarray, delta: float
    ) -> List[int]:
        """
        Select dimensions whose similarity scores fall within a margin
        of the maximum score.

        Args:
            scores:
                Similarity scores for a single question.
            delta:
                Margin threshold relative to the best score.

        Returns:
            List of dimension indices selected for this question.
        """
        max_score = scores.max()
        sorted_indices = np.argsort(-scores)

        return [
            i for i in sorted_indices
            if (max_score - scores[i]) <= delta
        ]
    
    def save_mapping_to_csv(
        self,
        mapped_df: pd.DataFrame,
        out_dir: str = "./temp_results",
        delta: Optional[float] = None,
    ) -> Path:
        """
        Save mapping results to a CSV file for downstream analysis.

        The output filename encodes:
            - dimension LLM name
            - embedding model name
            - margin (delta), if provided
            - timestamp

        Args:
            mapped_df:
                DataFrame returned by map_questions_to_dimensions().
            out_dir:
                Directory to save temporary results.
            delta:
                Margin threshold used in mapping (for traceability).

        Returns:
            Path to the saved CSV file.
        """
        out_dir = Path(out_dir)
        out_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        dim_model = self.dimension_model_name or "unknown-dim-model"

        delta_tag = f"delta{delta}" if delta is not None else "deltaNA"

        filename = (
            f"semantic_mapping__"
            f"dim_{dim_model}__"
            f"{delta_tag}__"
            f"{timestamp}.csv"
        )

        out_path = out_dir / filename
        mapped_df.to_csv(out_path, index=False)

        return out_path