{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57ce22e",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering of Mental Health Survey Questions\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements **Method 2 (Baseline): Unsupervised Clustering** for the mental health dimension reduction project.\n",
    "\n",
    "The goal of this method is to explore whether mental health survey questions exhibit **intrinsic semantic groupings** when embedded into a shared representation space, without imposing any predefined dimension labels or conceptual frameworks. Survey items are drawn from multiple validated instruments (e.g., PSQI, PSS, PWB, CD-RISC, UCLA Loneliness, PERMA) and treated as unlabeled text.\n",
    "\n",
    "Prior to embedding, questions are processed to remove recurrent grammatical patterns introduced by questionnaire design, ensuring that embeddings emphasize semantic content rather than stylistic regularities. All questions are then embedded using a pretrained sentence encoder and clustered using an unsupervised algorithm (**K-Means**). The resulting clusters represent latent semantic themes that emerge purely from distributional similarity.\n",
    "\n",
    "Cluster centers act as abstract representations of these latent themes, and representative questions are selected based on cosine similarity for qualitative inspection. This baseline provides a data-driven reference for comparison with concept-driven semantic mapping and supervised classification approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2594b",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "\n",
    "Each data point corresponds to a **single survey question**, represented with:\n",
    "- `qid`: the original item identifier (e.g., PSQI_5_3, PSS_12)\n",
    "- `dataset`: the source questionnaire or scale\n",
    "- `text`: the question text\n",
    "\n",
    "All questions are consolidated into a canonical dataset (`questions_master`) during preprocessing to ensure:\n",
    "- No modification of raw source files\n",
    "- Consistent identifiers across methods\n",
    "- Reproducibility across models and experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7402fa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CD_RISC_1</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>I am able to adapt when changes occur.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CD_RISC_2</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>I have one close and secure relationship.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CD_RISC_3</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>Sometimes fate or God helps me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CD_RISC_4</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>I can deal with whatever comes my way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CD_RISC_5</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>Past successes give me confidence.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid  dataset                                       text\n",
       "0  CD_RISC_1  CD-RISC     I am able to adapt when changes occur.\n",
       "1  CD_RISC_2  CD-RISC  I have one close and secure relationship.\n",
       "2  CD_RISC_3  CD-RISC            Sometimes fate or God helps me.\n",
       "3  CD_RISC_4  CD-RISC     I can deal with whatever comes my way.\n",
       "4  CD_RISC_5  CD-RISC         Past successes give me confidence."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_questions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "question = load_questions()\n",
    "question.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e2f4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "PWS        36\n",
       "CD-RISC    25\n",
       "PERMA      23\n",
       "PSS        23\n",
       "UCLA       20\n",
       "PWB        18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total questions:\", len(question))\n",
    "question[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e00af",
   "metadata": {},
   "source": [
    "## 3. Universal Dependencies (UD) Extraction with Stanza\n",
    "\n",
    "To mitigate the influence of recurrent grammatical patterns introduced by questionnaire design (e.g., repeated use of auxiliaries, determiners, or copular constructions), we leverage **Universal Dependencies (UD)** to obtain a syntactic view of each survey question.\n",
    "\n",
    "We use **Stanza**, a neural NLP toolkit, to parse each question into a UD dependency structure and extract the sequence of dependency relation labels (`deprel`) for all tokens, preserving the original word order.  \n",
    "This representation captures **syntactic roles rather than lexical content**, allowing us to identify and control for survey-specific structural templates.\n",
    "\n",
    "All questions are mapped to their corresponding UD dependency sequences and stored in a separate CSV file for inspection and downstream analysis. see \"./temp_result/UDmap.csv\"\n",
    "\n",
    "Universal Dependencies provide a standardized, cross-linguistic framework for syntactic annotation; see  \n",
    "https://universaldependencies.org/ for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6e3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haikeyu/Desktop/mentalhealth-dimension-reduction/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-04 14:37:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 170MB/s]                     \n",
      "2026-02-04 14:37:52 INFO: Downloaded file to /Users/haikeyu/stanza_resources/resources.json\n",
      "2026-02-04 14:37:52 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-02-04 14:37:53 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2026-02-04 14:37:53 INFO: Using device: cpu\n",
      "2026-02-04 14:37:53 INFO: Loading: tokenize\n",
      "2026-02-04 14:37:53 INFO: Loading: mwt\n",
      "2026-02-04 14:37:53 INFO: Loading: pos\n",
      "2026-02-04 14:37:54 INFO: Loading: lemma\n",
      "2026-02-04 14:37:55 INFO: Loading: depparse\n",
      "2026-02-04 14:37:55 INFO: Loading: ner\n",
      "2026-02-04 14:37:56 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to UDmap.csv\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "class UDExtractor:\n",
    "    def __init__(self, nlp: stanza.Pipeline):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def text_to_deprel(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert text to a space-separated UD deprel sequence.\n",
    "        Nothing is dropped. Order is preserved.\n",
    "        \"\"\"\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "\n",
    "        doc = self.nlp(str(text))\n",
    "        out = []\n",
    "\n",
    "        for sent in doc.sentences:\n",
    "            for w in sent.words:\n",
    "                out.append((w.deprel or \"dep\").lower())\n",
    "\n",
    "        return \" \".join(out)\n",
    "\n",
    "    def add_deprel_column(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        text_col: str = \"text\",\n",
    "        new_col: str = \"ud_deprel\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add a new column with UD deprel sequences.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        texts = df[text_col].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "        docs = self.nlp.bulk_process(texts)\n",
    "\n",
    "        deprels = []\n",
    "        for doc in docs:\n",
    "            seq = []\n",
    "            for sent in doc.sentences:\n",
    "                for w in sent.words:\n",
    "                    seq.append((w.deprel or \"dep\").lower())\n",
    "            deprels.append(\" \".join(seq))\n",
    "\n",
    "        df[new_col] = deprels\n",
    "        return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run\n",
    "# -----------------------------\n",
    "nlp = stanza.Pipeline(\n",
    "    \"en\",\n",
    "    processors=\"tokenize,pos,lemma,depparse,ner\",\n",
    "    tokenize_no_ssplit=True\n",
    ")\n",
    "\n",
    "df = question.copy()\n",
    "ud = UDExtractor(nlp)\n",
    "df = ud.add_deprel_column(df, text_col=\"text\", new_col=\"ud_deprel\")\n",
    "\n",
    "df.to_csv(\"./temp_result/UDmap.csv\", index=False)\n",
    "\n",
    "print(\"Saved to UDmap.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264aa013",
   "metadata": {},
   "source": [
    "## 4. Semantic Content Extraction Pipeline\n",
    "\n",
    "Before embedding and clustering, we apply a **syntax-aware semantic extraction pipeline** to reduce the influence of questionnaire-specific surface patterns and preserve the core semantic content of each item.\n",
    "\n",
    "### Motivation\n",
    "Mental health questionnaires often share recurring grammatical templates (e.g., *“I feel…”, “How often have you…”, “During the past month…”*).  \n",
    "If used directly, these patterns can dominate sentence embeddings and lead to clustering driven by **survey format rather than meaning**.\n",
    "\n",
    "### Method\n",
    "We use **Stanza** with Universal Dependencies (UD) parsing to perform token-level filtering based on syntactic roles and morphological features:\n",
    "\n",
    "- **Remove grammatical shells** such as auxiliaries, determiners, punctuation, copulas, and coordinating conjunctions.\n",
    "- **Drop subject personal pronouns** (e.g., *I, you, we*) to reduce first-/second-person framing effects, while retaining object or reflexive forms when semantically relevant.\n",
    "- **Remove all named-entity spans** (e.g., time, date, quantity expressions) to avoid anchoring on temporal or numeric questionnaire artifacts.\n",
    "- **Explicitly preserve negation** using UD morphological features (e.g., `Polarity=Neg`, `PronType=Neg`), ensuring that semantic polarity is retained.\n",
    "- Keep all remaining content-bearing tokens (verbs, nouns, adjectives, meaningful modifiers).\n",
    "\n",
    "Filtering is applied at the **token level**, and multi-word tokens (e.g., *“cannot”*) are preserved if any sub-token satisfies the retention criteria.\n",
    "\n",
    "### Output\n",
    "For each question, we produce:\n",
    "- A **cleaned semantic backbone** used for downstream embedding and clustering\n",
    "- The **original raw question text**, stored alongside the processed version for traceability\n",
    "\n",
    "This preprocessing step ensures that downstream semantic representations emphasize **conceptual meaning** rather than questionnaire structure, providing a more reliable foundation for unsupervised clustering and semantic analysis. see \"./temp_result/question_nvo.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d352f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from semantic_extractor import SemanticExtractor\n",
    "import stanza\n",
    "try:\n",
    "    nlp = stanza.Pipeline(\"en\", verbose=False)\n",
    "except:\n",
    "    stanza.download(\"en\")\n",
    "    nlp = stanza.Pipeline(\"en\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4697cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CD_RISC_1</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>able to adapt changes occur</td>\n",
       "      <td>I am able to adapt when changes occur.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CD_RISC_2</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>have close secure relationship</td>\n",
       "      <td>I have one close and secure relationship.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CD_RISC_3</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>fate god helps me</td>\n",
       "      <td>Sometimes fate or God helps me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CD_RISC_4</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>deal with whatever comes my way</td>\n",
       "      <td>I can deal with whatever comes my way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CD_RISC_5</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>past successes give me confidence</td>\n",
       "      <td>Past successes give me confidence.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid  dataset                               text  \\\n",
       "0  CD_RISC_1  CD-RISC        able to adapt changes occur   \n",
       "1  CD_RISC_2  CD-RISC     have close secure relationship   \n",
       "2  CD_RISC_3  CD-RISC                  fate god helps me   \n",
       "3  CD_RISC_4  CD-RISC    deal with whatever comes my way   \n",
       "4  CD_RISC_5  CD-RISC  past successes give me confidence   \n",
       "\n",
       "                                    text_raw  \n",
       "0     I am able to adapt when changes occur.  \n",
       "1  I have one close and secure relationship.  \n",
       "2            Sometimes fate or God helps me.  \n",
       "3     I can deal with whatever comes my way.  \n",
       "4         Past successes give me confidence.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_extractor = SemanticExtractor(nlp=nlp)\n",
    "question_se = semantic_extractor.transform_df(question, text_col=\"text\")\n",
    "question_se.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec0c57",
   "metadata": {},
   "source": [
    "## 5. Unsupervised Clustering Pipeline (Embedding → K-Means → Inspection)\n",
    "\n",
    "This section implements our **unsupervised clustering baseline** to examine whether survey questions form **intrinsic semantic groups** in embedding space, without using any predefined dimension labels.\n",
    "\n",
    "### Step 1 — Sentence Embedding\n",
    "We embed each (preprocessed) question using a pretrained sentence encoder (**Sentence-Transformers: `all-MiniLM-L6-v2`**).  \n",
    "Embeddings are **L2-normalized** (`normalize_embeddings=True`) so that cosine similarity reflects semantic proximity more consistently.\n",
    "\n",
    "### Step 2 — K-Means Clustering\n",
    "We apply **K-Means** with `k=8` to the embedding matrix and assign each question a `cluster_id`.  \n",
    "Cluster centroids serve as latent “theme prototypes” induced purely from distributional similarity.\n",
    "\n",
    "### Step 3 — Cluster Inspection (Representatives + Similarity Query)\n",
    "To interpret clusters, we:\n",
    "- Select **representative questions per cluster** by ranking items by cosine similarity to the cluster centroid (`sim_to_center`).\n",
    "- Support **nearest-neighbor querying** (`query_similar`) to retrieve the most semantically similar questions to a given item.\n",
    "\n",
    "### Step 4 — Export + Optional Human Naming\n",
    "Cluster assignments are saved to CSV in a **stable sorted order** (by `cluster_id`, then `dataset`, then `qid`).  \n",
    "Optionally, we provide a manual mapping from `cluster_id → cluster_type` (e.g., `[\"Sleep\", \"Spiritual\", ...]`) for downstream analysis and readability, and export the named results as an additional CSV. See \"./temp_result/named_cluster.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f0f5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>sim_to_center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSS_5_2</td>\n",
       "      <td>PSS</td>\n",
       "      <td>during had trouble sleeping because wake up in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.889706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PSS_5_10</td>\n",
       "      <td>PSS</td>\n",
       "      <td>during had trouble sleeping because of other r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.886029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PSS_5_9</td>\n",
       "      <td>PSS</td>\n",
       "      <td>during had trouble sleeping because have pain</td>\n",
       "      <td>0</td>\n",
       "      <td>0.824738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSS_5_1</td>\n",
       "      <td>PSS</td>\n",
       "      <td>during had trouble sleeping because cannot get...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.822696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PSS_5_8</td>\n",
       "      <td>PSS</td>\n",
       "      <td>during had trouble sleeping because have bad d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>CD_RISC_5</td>\n",
       "      <td>CD-RISC</td>\n",
       "      <td>past successes give me confidence</td>\n",
       "      <td>7</td>\n",
       "      <td>0.637245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>PERMA1_1</td>\n",
       "      <td>PERMA</td>\n",
       "      <td>much of time feel making progress towards acco...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.636630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>PWS_31</td>\n",
       "      <td>PWS</td>\n",
       "      <td>things not work out way want them to in future</td>\n",
       "      <td>7</td>\n",
       "      <td>0.634175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>PWB_7</td>\n",
       "      <td>PWB</td>\n",
       "      <td>live life at time don't think about future</td>\n",
       "      <td>7</td>\n",
       "      <td>0.612619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>PWB_2</td>\n",
       "      <td>PWB</td>\n",
       "      <td>look at story of my life pleased with how thin...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.602191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid  dataset                                               text  \\\n",
       "0     PSS_5_2      PSS  during had trouble sleeping because wake up in...   \n",
       "1    PSS_5_10      PSS  during had trouble sleeping because of other r...   \n",
       "2     PSS_5_9      PSS      during had trouble sleeping because have pain   \n",
       "3     PSS_5_1      PSS  during had trouble sleeping because cannot get...   \n",
       "4     PSS_5_8      PSS  during had trouble sleeping because have bad d...   \n",
       "..        ...      ...                                                ...   \n",
       "74  CD_RISC_5  CD-RISC                  past successes give me confidence   \n",
       "75   PERMA1_1    PERMA  much of time feel making progress towards acco...   \n",
       "76     PWS_31      PWS     things not work out way want them to in future   \n",
       "77      PWB_7      PWB         live life at time don't think about future   \n",
       "78      PWB_2      PWB  look at story of my life pleased with how thin...   \n",
       "\n",
       "    cluster_id  sim_to_center  \n",
       "0            0       0.889706  \n",
       "1            0       0.886029  \n",
       "2            0       0.824738  \n",
       "3            0       0.822696  \n",
       "4            0       0.821908  \n",
       "..         ...            ...  \n",
       "74           7       0.637245  \n",
       "75           7       0.636630  \n",
       "76           7       0.634175  \n",
       "77           7       0.612619  \n",
       "78           7       0.602191  \n",
       "\n",
       "[79 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('temp_result/cluster.csv')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from semantic_cluster import SemanticCluster\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "clusterer = SemanticCluster(embedder, k=8)\n",
    "\n",
    "df_clustered = clusterer.fit(question_se, text_col=\"text\")\n",
    "\n",
    "\n",
    "rep_df = clusterer.get_representatives(top_n=10)\n",
    "display(rep_df)\n",
    "\n",
    "\n",
    "clusterer.query_similar(i=10, top_k=6)\n",
    "\n",
    "clusterer.save_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce81397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('temp_result/named_cluster.csv')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_name = [\"Sleep\", \"Spiritual\", \"Relationship\", \"Relationship\", \"Feeling\", \"Physical\", \"Challenge\", \"Future\"]\n",
    "clusterer.map_cluster_type(cluster_name)\n",
    "clusterer.save_cluster(\"./temp_result/named_cluster.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5b8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18c00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da868d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
